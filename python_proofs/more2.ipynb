{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Liquid Time Constant Neuron\n",
    "class LTCNeuron:\n",
    "    def __init__(self, tau=1.0, alpha=0.1):\n",
    "        self.state = 0.0\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def update(self, input_signal, dt=0.1):\n",
    "        def g(h): return -math.tanh(h)  # Nonlinear attractor function\n",
    "        dstate = (1 / self.tau) * (input_signal - self.state) + self.alpha * g(self.state)\n",
    "        self.state += dstate * dt\n",
    "        return self.state\n",
    "\n",
    "\n",
    "# Boltzmann Selector\n",
    "class BoltzmannSelector:\n",
    "    def __init__(self, temperature=1.0):\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def select(self, candidates, energy_func):\n",
    "        energies = np.array([energy_func(c) for c in candidates])\n",
    "        probabilities = np.exp(-energies / self.temperature)\n",
    "        probabilities /= probabilities.sum()  # Normalize\n",
    "        return np.random.choice(candidates, p=probabilities)\n",
    "\n",
    "\n",
    "# Curiosity Mechanism\n",
    "class CuriosityMechanism:\n",
    "    def __init__(self, curiosity_weight=0.5):\n",
    "        self.novelty_scores = defaultdict(float)\n",
    "        self.curiosity_weight = curiosity_weight\n",
    "\n",
    "    def update_curiosity(self, token, prediction_error):\n",
    "        self.novelty_scores[token] += self.curiosity_weight * prediction_error\n",
    "\n",
    "    def get_curiosity(self, token):\n",
    "        return self.novelty_scores[token]\n",
    "\n",
    "\n",
    "# Prefrontal Cortex\n",
    "class PrefrontalCortex:\n",
    "    def __init__(self):\n",
    "        self.timeline = defaultdict(list)  # Time-indexed neuron activity\n",
    "        self.replay_buffer = []  # Buffer for replaying patterns\n",
    "\n",
    "    def record_state(self, t, neuron_states):\n",
    "        \"\"\"\n",
    "        Record the state of neurons at time t.\n",
    "        \"\"\"\n",
    "        self.timeline[t] = neuron_states\n",
    "\n",
    "    def replay(self, start_t, end_t):\n",
    "        \"\"\"\n",
    "        Replay states between start_t and end_t to reinforce patterns.\n",
    "        \"\"\"\n",
    "        return [self.timeline[t] for t in range(start_t, end_t + 1) if t in self.timeline]\n",
    "\n",
    "\n",
    "# Layered Language Processor\n",
    "class LayeredLanguageProcessor:\n",
    "    def __init__(self, num_layers=3, neurons_per_layer=5, temperature=1.0, curiosity_weight=0.5):\n",
    "        \"\"\"\n",
    "        A multi-layer processor using chained LTC neurons.\n",
    "        \"\"\"\n",
    "        self.layers = [\n",
    "            [LTCNeuron(tau=1.0 + i * 0.2) for i in range(neurons_per_layer)]\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.selector = BoltzmannSelector(temperature=temperature)\n",
    "        self.curiosity = CuriosityMechanism(curiosity_weight=curiosity_weight)\n",
    "        self.pfc = PrefrontalCortex()  # Shared PFC for temporal binding\n",
    "        self.time_index = 0\n",
    "\n",
    "    def process_stream(self, input_stream):\n",
    "        outputs = []\n",
    "        last_time = time.time()\n",
    "\n",
    "        for input_char in input_stream:\n",
    "            # Wait for next real-time input\n",
    "            current_time = time.time()\n",
    "            dt = current_time - last_time\n",
    "            last_time = current_time\n",
    "\n",
    "            input_signal = ord(input_char) / 255.0  # Normalize input\n",
    "            layer_outputs = [input_signal]  # Input propagates through layers\n",
    "\n",
    "            # Pass through each layer\n",
    "            for layer in self.layers:\n",
    "                layer_states = []\n",
    "                for neuron in layer:\n",
    "                    state = neuron.update(layer_outputs[-1], dt)\n",
    "                    layer_states.append(state)\n",
    "                layer_outputs.append(sum(layer_states) / len(layer_states))  # Aggregate layer output\n",
    "\n",
    "            # Compute energy (inverse curiosity) for focus selection\n",
    "            def energy_func(token): return 1 - self.curiosity.get_curiosity(token)\n",
    "            focus = self.selector.select([input_char], energy_func)\n",
    "\n",
    "            # Update curiosity\n",
    "            prediction_error = abs(input_signal - ord(focus) / 255.0)\n",
    "            self.curiosity.update_curiosity(focus, prediction_error)\n",
    "\n",
    "            # Record neuron states in the PFC\n",
    "            self.pfc.record_state(self.time_index, layer_outputs)\n",
    "\n",
    "            # Record output\n",
    "            outputs.append({\"focus\": focus, \"layer_outputs\": layer_outputs})\n",
    "            self.time_index += 1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Main Program\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the document\n",
    "    document_path = \"teachbaby.txt\"  # Replace with the path to your document\n",
    "    with open(document_path, 'r', encoding='utf-8') as f:\n",
    "        text_stream = f.read()\n",
    "\n",
    "    # Initialize processor\n",
    "    processor = LayeredLanguageProcessor(num_layers=3, neurons_per_layer=5, temperature=0.5, curiosity_weight=0.2)\n",
    "\n",
    "    # Process the document as a stream\n",
    "    outputs = processor.process_stream(text_stream)\n",
    "\n",
    "    for t, result in enumerate(outputs):  # Print all outputs\n",
    "        print(f\"Time {t}: Focus = {result['focus']}, Layer Outputs = {result['layer_outputs']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_layer_outputs(outputs, layers_to_plot=[1, 2, 3]):\n",
    "    for layer_idx in layers_to_plot:\n",
    "        layer_data = [output['layer_outputs'][layer_idx] for output in outputs]\n",
    "        plt.plot(layer_data, label=f'Layer {layer_idx}')\n",
    "    \n",
    "    plt.title('Layer Outputs Over Time')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('Layer Output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_layer_outputs(outputs, layers_to_plot=[1, 2, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_word_patterns(outputs, input_stream, window_size=5, layer=3):\n",
    "    \"\"\"\n",
    "    Analyze deeper layer outputs for word-like patterns using a sliding window.\n",
    "    \"\"\"\n",
    "    word_patterns = {}\n",
    "    input_length = len(input_stream)\n",
    "    \n",
    "    for i in range(input_length - window_size + 1):\n",
    "        # Extract a sliding window of Layer 3 outputs\n",
    "        window_outputs = [outputs[j]['layer_outputs'][layer] for j in range(i, i + window_size)]\n",
    "        \n",
    "        # Concatenate input characters for reference\n",
    "        input_segment = ''.join(input_stream[i:i + window_size])\n",
    "        \n",
    "        # Store the pattern and its corresponding input segment\n",
    "        word_patterns[input_segment] = window_outputs\n",
    "    \n",
    "    return word_patterns\n",
    "\n",
    "\n",
    "# Analyze word-like patterns using a sliding window\n",
    "word_patterns = analyze_word_patterns(outputs, text_stream, window_size=5)\n",
    "\n",
    "# Print a sample of patterns\n",
    "print(\"Word Patterns (Sample):\")\n",
    "for i, (segment, pattern) in enumerate(word_patterns.items()):\n",
    "    if i < 10:  # Limit to first 10 for brevity\n",
    "        print(f\"Input: {segment}, Layer Outputs: {pattern}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "def cluster_layer_outputs(outputs, layer=3, n_clusters=10):\n",
    "    \"\"\"\n",
    "    Cluster the outputs of a specified layer using k-means.\n",
    "    \"\"\"\n",
    "    # Extract Layer 3 outputs\n",
    "    layer_outputs = [output['layer_outputs'][layer] for output in outputs]\n",
    "    layer_outputs = np.array(layer_outputs).reshape(-1, 1)  # Reshape for clustering\n",
    "    \n",
    "    # Perform k-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(layer_outputs)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "# Cluster Layer 3 outputs\n",
    "cluster_labels = cluster_layer_outputs(outputs, layer=3)\n",
    "\n",
    "# Print a sample of cluster assignments\n",
    "print(\"Cluster Assignments (Sample):\")\n",
    "for i, label in enumerate(cluster_labels[:100]):  # Limit to first 100\n",
    "    print(f\"Timestep {i}: Cluster {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_word_boundaries(outputs, layer=3, threshold=1e-10):\n",
    "    \"\"\"\n",
    "    Detect word boundaries based on drops or spikes in deeper layer activations.\n",
    "    \"\"\"\n",
    "    boundaries = []\n",
    "    prev_value = outputs[0]['layer_outputs'][layer]\n",
    "    \n",
    "    for t, output in enumerate(outputs[1:], start=1):\n",
    "        current_value = output['layer_outputs'][layer]\n",
    "        if abs(current_value - prev_value) > threshold:\n",
    "            boundaries.append(t)  # Mark a boundary\n",
    "        prev_value = current_value\n",
    "    \n",
    "    return boundaries\n",
    "\n",
    "# Detect boundaries in Layer 3 outputs\n",
    "word_boundaries = detect_word_boundaries(outputs, layer=3)\n",
    "\n",
    "# Print detected boundaries\n",
    "print(\"Detected Word Boundaries:\")\n",
    "print(word_boundaries)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_outputs(outputs, layers_to_plot=[1, 2, 3])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
