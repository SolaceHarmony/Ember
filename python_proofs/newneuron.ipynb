{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 (Modified)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "\n",
    "@dataclass\n",
    "class AttentionState:\n",
    "    temporal_weight: float = 0.0\n",
    "    causal_weight: float = 0.0\n",
    "    novelty_weight: float = 0.0\n",
    "    \n",
    "    def compute_total(self) -> float:\n",
    "        return (self.temporal_weight + self.causal_weight + self.novelty_weight) / 3.0\n",
    "\n",
    "class CausalAttention:\n",
    "    def __init__(self, decay_rate=0.1, novelty_threshold=0.3, memory_length=100):\n",
    "        self.states: Dict[int, AttentionState] = {}\n",
    "        self.history: List[Tuple[int, float]] = []\n",
    "        self.decay_rate = decay_rate\n",
    "        self.novelty_threshold = novelty_threshold\n",
    "        self.memory_length = memory_length\n",
    "    \n",
    "    def update(self, neuron_id: int, prediction_error: float, current_state: float, target_state: float) -> float:\n",
    "        state = self.states.get(neuron_id, AttentionState())\n",
    "\n",
    "        # Temporal\n",
    "        temporal_decay = np.exp(-self.decay_rate * len(self.history))\n",
    "        state.temporal_weight = current_state * temporal_decay\n",
    "        \n",
    "        # Causal\n",
    "        prediction_accuracy = 1.0 - min(abs(prediction_error), 1.0)\n",
    "        state.causal_weight = prediction_accuracy\n",
    "        \n",
    "        # Novelty\n",
    "        novelty = abs(target_state - current_state)\n",
    "        if novelty > self.novelty_threshold:\n",
    "            state.novelty_weight = novelty\n",
    "        else:\n",
    "            state.novelty_weight *= (1 - self.decay_rate)\n",
    "        \n",
    "        self.states[neuron_id] = state\n",
    "        total_attention = state.compute_total()\n",
    "        self.history.append((neuron_id, total_attention))\n",
    "        if len(self.history) > self.memory_length:\n",
    "            self.history.pop(0)\n",
    "        \n",
    "        return total_attention\n",
    "\n",
    "class HebbianLayer:\n",
    "    def __init__(self, input_size, output_size, eta=0.01, recurrent_eta=0.005):\n",
    "        self.weights = np.random.randn(output_size, input_size) * 0.01\n",
    "        self.recurrent_weights = np.random.randn(output_size, output_size) * 0.01 # Add recurrent weights\n",
    "        self.eta = eta\n",
    "        self.recurrent_eta = recurrent_eta\n",
    "        self.last_output = np.zeros(output_size)  # Store the last output for recurrent input\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        feedforward_output = self.weights @ inputs\n",
    "        recurrent_output = self.recurrent_weights @ self.last_output # Add recurrent input\n",
    "        self.last_output = feedforward_output + recurrent_output # Update last_output\n",
    "        return self.last_output\n",
    "\n",
    "    def hebbian_update(self, inputs: np.ndarray, outputs: np.ndarray):\n",
    "        delta_w = self.eta * np.outer(outputs, inputs)\n",
    "        self.weights += delta_w\n",
    "        \n",
    "        # Update recurrent weights using Hebbian rule\n",
    "        delta_w_recurrent = self.recurrent_eta * np.outer(outputs, self.last_output)\n",
    "        self.recurrent_weights += delta_w_recurrent\n",
    "\n",
    "class DopamineModulator:\n",
    "    \"\"\"\n",
    "    Simple dopamine model that increases with strong input and decays over time.\n",
    "    Dopamine affects the neuron's effective time constant to slow decay when high.\n",
    "    \"\"\"\n",
    "    def __init__(self, increase_rate=0.05, decay_rate=0.01):\n",
    "        self.dopamine_level = 0.0\n",
    "        self.increase_rate = increase_rate\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "    def update(self, input_strength: float):\n",
    "        # Increase dopamine if input is strong (above 0.5 for example)\n",
    "        if input_strength > 0.5:\n",
    "            self.dopamine_level += self.increase_rate * (input_strength - 0.5)\n",
    "        # Decay dopamine level\n",
    "        self.dopamine_level = max(0.0, self.dopamine_level - self.decay_rate)\n",
    "\n",
    "    def get_dopamine_modulation(self):\n",
    "        # Higher dopamine -> larger effective tau (less decay)\n",
    "        return 1.0 + self.dopamine_level\n",
    "\n",
    "class WeightedLTCNeuron:\n",
    "    def __init__(self, tau=1.0, num_inputs=3):\n",
    "        self.state = 0.0\n",
    "        self.base_tau = tau\n",
    "        self.num_inputs = num_inputs\n",
    "        self.weights = np.random.uniform(-0.5, 0.5, size=num_inputs)\n",
    "    \n",
    "    def update(self, inputs, dt=0.1, tau_mod=1.0, feedback=0.0):\n",
    "        # feedback is a recurrent input from another neuron\n",
    "        synaptic_input = np.dot(self.weights, inputs) + feedback\n",
    "        effective_tau = self.base_tau * tau_mod\n",
    "        dstate = (-self.state / effective_tau) + synaptic_input\n",
    "        self.state += dstate * dt\n",
    "        return self.state\n",
    "\n",
    "class SpecializedNeuron(WeightedLTCNeuron):\n",
    "    def __init__(self, tau=1.0, role=\"default\", num_inputs=3):\n",
    "        super().__init__(tau=tau, num_inputs=num_inputs)\n",
    "        self.role = role\n",
    "\n",
    "    def update(self, inputs, dt=0.1, tau_mod=1.0, feedback=0.0):\n",
    "        synaptic_input = np.dot(self.weights, inputs) + feedback\n",
    "        \n",
    "        if self.role == \"memory\":\n",
    "            # Slower decay baseline\n",
    "            effective_tau = self.base_tau * tau_mod * 1.5\n",
    "            dstate = (-self.state / effective_tau) + synaptic_input\n",
    "        elif self.role == \"inhibition\":\n",
    "            # Inhibit signals\n",
    "            synaptic_input = -synaptic_input * 0.5\n",
    "            effective_tau = self.base_tau * tau_mod\n",
    "            dstate = (-self.state / effective_tau) + synaptic_input\n",
    "        elif self.role == \"amplification\":\n",
    "            # Amplify signals\n",
    "            synaptic_input = synaptic_input * 1.5\n",
    "            effective_tau = self.base_tau * tau_mod\n",
    "            dstate = (-self.state / effective_tau) + synaptic_input\n",
    "        else:\n",
    "            # Default LTC behavior\n",
    "            effective_tau = self.base_tau * tau_mod\n",
    "            dstate = (-self.state / effective_tau) + synaptic_input\n",
    "        \n",
    "        self.state += dstate * dt\n",
    "        return self.state\n",
    "\n",
    "class LTCNeuronWithAttention:\n",
    "    def __init__(self, neuron_id: int, tau: float = 1.0):\n",
    "        self.id = neuron_id\n",
    "        self.tau = tau\n",
    "        self.state = 0.0\n",
    "        self.attention = CausalAttention()\n",
    "        self.last_prediction = 0.0\n",
    "        \n",
    "    def update(self, input_signal: float, dt: float) -> float:\n",
    "        prediction_error = input_signal - self.last_prediction\n",
    "        attention_value = self.attention.update(\n",
    "            neuron_id=self.id,\n",
    "            prediction_error=prediction_error,\n",
    "            current_state=self.state,\n",
    "            target_state=input_signal\n",
    "        )\n",
    "        effective_tau = self.tau * (1.0 - 0.3 * attention_value)\n",
    "        \n",
    "        d_state = (1.0/effective_tau) * ((input_signal * (1.0 + attention_value)) - self.state) * dt\n",
    "        self.state += d_state\n",
    "        self.last_prediction = self.state\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 (Modified)\n",
    "class AttentiveNetworkWithHebbAndRoles:\n",
    "    def __init__(self, roles=None, input_size=3, eta=0.005, dopamine_increase_rate=0.05, dopamine_decay_rate=0.01, recurrent_eta=0.002): #Added recurrent_eta\n",
    "        if roles is None:\n",
    "            # Default roles: one memory, one inhibition, one amplification\n",
    "            roles = [\"memory\", \"inhibition\", \"amplification\"]\n",
    "        \n",
    "        self.neurons = [\n",
    "            SpecializedNeuron(tau=1.0+(0.2*i), role=roles[i % len(roles)], num_inputs=input_size)\n",
    "            for i in range(len(roles))\n",
    "        ]\n",
    "        \n",
    "        self.num_neurons = len(self.neurons)\n",
    "        self.hebb = HebbianLayer(input_size=input_size, output_size=self.num_neurons, eta=eta, recurrent_eta=recurrent_eta) # Initialize with recurrent_eta\n",
    "        self.dopamine = DopamineModulator(increase_rate=dopamine_increase_rate, decay_rate=dopamine_decay_rate)\n",
    "        \n",
    "        self.state_history = []\n",
    "        self.input_history = []\n",
    "        self.time_steps = []\n",
    "        self.current_step = 0\n",
    "\n",
    "    def update_step(self, raw_inputs: List[float], dt=0.1):\n",
    "        inputs = np.array(raw_inputs)\n",
    "        \n",
    "        # Update dopamine based on input strength (e.g., mean of inputs)\n",
    "        input_strength = np.mean(inputs)\n",
    "        self.dopamine.update(input_strength)\n",
    "        tau_mod = self.dopamine.get_dopamine_modulation()\n",
    "        \n",
    "        # Hebbian forward pass\n",
    "        hebb_outputs = self.hebb.forward(inputs) # Pass the recurrent input into the Hebbian layer\n",
    "        \n",
    "        # Simple recurrent feedback: let's feed the last neuron's state as feedback to the first neuron\n",
    "        feedback = self.neurons[-1].state * 0.1  # small feedback factor\n",
    "        \n",
    "        states = []\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            fb = feedback if i == 0 else 0.0  # Only first neuron receives feedback from last neuron\n",
    "            s = neuron.update(hebb_outputs, dt, tau_mod=tau_mod, feedback=fb)\n",
    "            states.append(s)\n",
    "        \n",
    "        # Hebbian update\n",
    "        pre_activities = inputs\n",
    "        post_activities = np.array(states)\n",
    "        self.hebb.hebbian_update(pre_activities, post_activities)\n",
    "        \n",
    "        self.state_history.append(states)\n",
    "        self.input_history.append(raw_inputs)\n",
    "        self.time_steps.append(self.current_step)\n",
    "        self.current_step += 1\n",
    "        \n",
    "        return states\n",
    "\n",
    "    def plot_results(self):\n",
    "        states = np.array(self.state_history)\n",
    "        inputs = np.array(self.input_history)\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(2,1,figsize=(12,8))\n",
    "\n",
    "        # States\n",
    "        for i in range(self.num_neurons):\n",
    "            ax1.plot(self.time_steps, states[:, i], label=f'Neuron {i} ({self.neurons[i].role})')\n",
    "        ax1.set_title('Neuron States')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        # Inputs\n",
    "        for i in range(inputs.shape[1]):\n",
    "            ax2.plot(self.time_steps, inputs[:, i], label=f'Input {i}')\n",
    "        ax2.set_title('Inputs')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_hebb_weights(self):\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.imshow(self.hebb.weights, aspect='auto', cmap='viridis')\n",
    "        plt.colorbar(label='Weight value')\n",
    "        plt.xlabel('Input Units')\n",
    "        plt.ylabel('Output Neurons')\n",
    "        plt.title('Hebbian Weights')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_recurrent_hebb_weights(self):\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.imshow(self.hebb.recurrent_weights, aspect='auto', cmap='viridis')\n",
    "        plt.colorbar(label='Recurrent Weight value')\n",
    "        plt.xlabel('Output Neurons (from)')\n",
    "        plt.ylabel('Output Neurons (to)')\n",
    "        plt.title('Hebbian Recurrent Weights')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Time Step: 0\n",
      "Character: 'T' (ord=84, normalized=0.3294)\n",
      "dt: 0.0004s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0000\n",
      "Input to Hebbian layer: [0.32941176470588235, 0.16470588235294117, -0.03411764705882353]\n",
      "Hebbian outputs (to neurons): [ 0.005206   -0.00186759 -0.00348172]\n",
      "Neuron 0 (memory): syn=-0.0015, tau=1.5000\n",
      "Neuron 0 (memory): old_state=0.0000, new_state=-0.0000, dstate=-0.0015\n",
      "Neuron 1 (inhibition): original syn=-0.0022, inhibited syn=0.0011, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0000, new_state=0.0000, dstate=0.0011\n",
      "Neuron 2 (amplification): original syn=0.0015, amplified syn=0.0022, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0000, new_state=0.0000, dstate=0.0022\n",
      "==================================================\n",
      "Time Step: 1\n",
      "Character: 'h' (ord=104, normalized=0.4078)\n",
      "dt: 0.2092s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0000\n",
      "Input to Hebbian layer: [0.40784313725490196, 0.20392156862745098, -0.01843137254901961]\n",
      "Hebbian outputs (to neurons): [ 0.00648365 -0.00226007 -0.00390717]\n",
      "Neuron 0 (memory): syn=-0.0020, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0000, new_state=-0.0004, dstate=-0.0020\n",
      "Neuron 1 (inhibition): original syn=-0.0025, inhibited syn=0.0013, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0000, new_state=0.0003, dstate=0.0013\n",
      "Neuron 2 (amplification): original syn=0.0019, amplified syn=0.0028, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0000, new_state=0.0006, dstate=0.0028\n",
      "==================================================\n",
      "Time Step: 2\n",
      "Character: 'e' (ord=101, normalized=0.3961)\n",
      "dt: 0.2265s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0001\n",
      "Input to Hebbian layer: [0.396078431372549, 0.1980392156862745, -0.0207843137254902]\n",
      "Hebbian outputs (to neurons): [ 0.00630423 -0.00218981 -0.00384571]\n",
      "Neuron 0 (memory): syn=-0.0018, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0004, new_state=-0.0008, dstate=-0.0016\n",
      "Neuron 1 (inhibition): original syn=-0.0025, inhibited syn=0.0012, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0003, new_state=0.0005, dstate=0.0010\n",
      "Neuron 2 (amplification): original syn=0.0018, amplified syn=0.0027, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0006, new_state=0.0011, dstate=0.0023\n",
      "==================================================\n",
      "Time Step: 3\n",
      "Character: ' ' (ord=32, normalized=0.1255)\n",
      "dt: 0.2351s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0001\n",
      "Input to Hebbian layer: [0.12549019607843137, 0.06274509803921569, -0.07490196078431373]\n",
      "Hebbian outputs (to neurons): [ 0.00200802 -0.00069153 -0.00238609]\n",
      "Neuron 0 (memory): syn=-0.0001, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0008, new_state=-0.0007, dstate=0.0004\n",
      "Neuron 1 (inhibition): original syn=-0.0012, inhibited syn=0.0006, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0005, new_state=0.0005, dstate=0.0002\n",
      "Neuron 2 (amplification): original syn=0.0005, amplified syn=0.0008, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0011, new_state=0.0011, dstate=0.0000\n",
      "==================================================\n",
      "Time Step: 4\n",
      "Character: 'q' (ord=113, normalized=0.4431)\n",
      "dt: 0.2226s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0001\n",
      "Input to Hebbian layer: [0.44313725490196076, 0.22156862745098038, -0.011372549019607847]\n",
      "Hebbian outputs (to neurons): [ 0.00702456 -0.00246672 -0.00408786]\n",
      "Neuron 0 (memory): syn=-0.0021, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0007, new_state=-0.0010, dstate=-0.0016\n",
      "Neuron 1 (inhibition): original syn=-0.0027, inhibited syn=0.0014, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0005, new_state=0.0007, dstate=0.0009\n",
      "Neuron 2 (amplification): original syn=0.0020, amplified syn=0.0030, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0011, new_state=0.0016, dstate=0.0023\n",
      "==================================================\n",
      "Time Step: 5\n",
      "Character: 'u' (ord=117, normalized=0.4588)\n",
      "dt: 0.2218s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0002\n",
      "Input to Hebbian layer: [0.4588235294117647, 0.22941176470588234, -0.008235294117647063]\n",
      "Hebbian outputs (to neurons): [ 0.00730065 -0.00253368 -0.00418158]\n",
      "Neuron 0 (memory): syn=-0.0021, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0010, new_state=-0.0014, dstate=-0.0014\n",
      "Neuron 1 (inhibition): original syn=-0.0028, inhibited syn=0.0014, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0007, new_state=0.0009, dstate=0.0008\n",
      "Neuron 2 (amplification): original syn=0.0021, amplified syn=0.0032, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0016, new_state=0.0021, dstate=0.0020\n",
      "==================================================\n",
      "Time Step: 6\n",
      "Character: 'i' (ord=105, normalized=0.4118)\n",
      "dt: 0.2105s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0002\n",
      "Input to Hebbian layer: [0.4117647058823529, 0.20588235294117646, -0.017647058823529415]\n",
      "Hebbian outputs (to neurons): [ 0.00655417 -0.00227102 -0.00392626]\n",
      "Neuron 0 (memory): syn=-0.0018, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0014, new_state=-0.0015, dstate=-0.0009\n",
      "Neuron 1 (inhibition): original syn=-0.0025, inhibited syn=0.0013, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0009, new_state=0.0010, dstate=0.0005\n",
      "Neuron 2 (amplification): original syn=0.0019, amplified syn=0.0028, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0021, new_state=0.0024, dstate=0.0014\n",
      "==================================================\n",
      "Time Step: 7\n",
      "Character: 'c' (ord=99, normalized=0.3882)\n",
      "dt: 0.2195s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0002\n",
      "Input to Hebbian layer: [0.38823529411764707, 0.19411764705882353, -0.02235294117647059]\n",
      "Hebbian outputs (to neurons): [ 0.0061751  -0.00214267 -0.00379572]\n",
      "Neuron 0 (memory): syn=-0.0016, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0015, new_state=-0.0017, dstate=-0.0006\n",
      "Neuron 1 (inhibition): original syn=-0.0024, inhibited syn=0.0012, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0010, new_state=0.0011, dstate=0.0004\n",
      "Neuron 2 (amplification): original syn=0.0018, amplified syn=0.0027, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0024, new_state=0.0026, dstate=0.0010\n",
      "==================================================\n",
      "Time Step: 8\n",
      "Character: 'k' (ord=107, normalized=0.4196)\n",
      "dt: 0.2176s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0003\n",
      "Input to Hebbian layer: [0.4196078431372549, 0.20980392156862746, -0.01607843137254902]\n",
      "Hebbian outputs (to neurons): [ 0.00666874 -0.00231651 -0.00396081]\n",
      "Neuron 0 (memory): syn=-0.0018, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0017, new_state=-0.0018, dstate=-0.0007\n",
      "Neuron 1 (inhibition): original syn=-0.0026, inhibited syn=0.0013, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0011, new_state=0.0012, dstate=0.0004\n",
      "Neuron 2 (amplification): original syn=0.0019, amplified syn=0.0029, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0026, new_state=0.0028, dstate=0.0011\n",
      "==================================================\n",
      "Time Step: 9\n",
      "Character: ' ' (ord=32, normalized=0.1255)\n",
      "dt: 0.2227s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0003\n",
      "Input to Hebbian layer: [0.12549019607843137, 0.06274509803921569, -0.07490196078431373]\n",
      "Hebbian outputs (to neurons): [ 0.0020075  -0.00068853 -0.00238296]\n",
      "Neuron 0 (memory): syn=0.0000, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0018, new_state=-0.0015, dstate=0.0013\n",
      "Neuron 1 (inhibition): original syn=-0.0012, inhibited syn=0.0006, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0012, new_state=0.0011, dstate=-0.0004\n",
      "Neuron 2 (amplification): original syn=0.0005, amplified syn=0.0008, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0028, new_state=0.0025, dstate=-0.0012\n",
      "==================================================\n",
      "Time Step: 10\n",
      "Character: 'b' (ord=98, normalized=0.3843)\n",
      "dt: 0.2135s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0003\n",
      "Input to Hebbian layer: [0.3843137254901961, 0.19215686274509805, -0.02313725490196078]\n",
      "Hebbian outputs (to neurons): [ 0.00608298 -0.00213566 -0.00375858]\n",
      "Neuron 0 (memory): syn=-0.0016, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0015, new_state=-0.0017, dstate=-0.0006\n",
      "Neuron 1 (inhibition): original syn=-0.0024, inhibited syn=0.0012, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0011, new_state=0.0012, dstate=0.0003\n",
      "Neuron 2 (amplification): original syn=0.0018, amplified syn=0.0026, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0025, new_state=0.0027, dstate=0.0008\n",
      "==================================================\n",
      "Time Step: 11\n",
      "Character: 'r' (ord=114, normalized=0.4471)\n",
      "dt: 0.2096s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0003\n",
      "Input to Hebbian layer: [0.4470588235294118, 0.2235294117647059, -0.010588235294117643]\n",
      "Hebbian outputs (to neurons): [ 0.00709877 -0.0024657  -0.00410095]\n",
      "Neuron 0 (memory): syn=-0.0019, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0017, new_state=-0.0018, dstate=-0.0008\n",
      "Neuron 1 (inhibition): original syn=-0.0027, inhibited syn=0.0014, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0012, new_state=0.0012, dstate=0.0004\n",
      "Neuron 2 (amplification): original syn=0.0021, amplified syn=0.0031, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0027, new_state=0.0030, dstate=0.0012\n",
      "==================================================\n",
      "Time Step: 12\n",
      "Character: 'o' (ord=111, normalized=0.4353)\n",
      "dt: 0.2220s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0003\n",
      "Input to Hebbian layer: [0.43529411764705883, 0.21764705882352942, -0.012941176470588234]\n",
      "Hebbian outputs (to neurons): [ 0.00691612 -0.00239538 -0.00403656]\n",
      "Neuron 0 (memory): syn=-0.0018, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0018, new_state=-0.0020, dstate=-0.0006\n",
      "Neuron 1 (inhibition): original syn=-0.0027, inhibited syn=0.0013, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0012, new_state=0.0013, dstate=0.0003\n",
      "Neuron 2 (amplification): original syn=0.0020, amplified syn=0.0030, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0030, new_state=0.0032, dstate=0.0009\n",
      "==================================================\n",
      "Time Step: 13\n",
      "Character: 'w' (ord=119, normalized=0.4667)\n",
      "dt: 0.2172s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0003\n",
      "Input to Hebbian layer: [0.4666666666666667, 0.23333333333333334, -0.006666666666666665]\n",
      "Hebbian outputs (to neurons): [ 0.00740958 -0.00256754 -0.0041997 ]\n",
      "Neuron 0 (memory): syn=-0.0020, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0020, new_state=-0.0021, dstate=-0.0007\n",
      "Neuron 1 (inhibition): original syn=-0.0028, inhibited syn=0.0014, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0013, new_state=0.0014, dstate=0.0003\n",
      "Neuron 2 (amplification): original syn=0.0022, amplified syn=0.0032, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0032, new_state=0.0034, dstate=0.0010\n",
      "==================================================\n",
      "Time Step: 14\n",
      "Character: 'n' (ord=110, normalized=0.4314)\n",
      "dt: 0.2277s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0003\n",
      "Input to Hebbian layer: [0.43137254901960786, 0.21568627450980393, -0.013725490196078428]\n",
      "Hebbian outputs (to neurons): [ 0.00685082 -0.00236939 -0.00400836]\n",
      "Neuron 0 (memory): syn=-0.0018, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0021, new_state=-0.0022, dstate=-0.0004\n",
      "Neuron 1 (inhibition): original syn=-0.0026, inhibited syn=0.0013, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0014, new_state=0.0014, dstate=0.0002\n",
      "Neuron 2 (amplification): original syn=0.0020, amplified syn=0.0030, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0034, new_state=0.0035, dstate=0.0006\n",
      "==================================================\n",
      "Time Step: 15\n",
      "Character: ' ' (ord=32, normalized=0.1255)\n",
      "dt: 0.2121s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0003\n",
      "Input to Hebbian layer: [0.12549019607843137, 0.06274509803921569, -0.07490196078431373]\n",
      "Hebbian outputs (to neurons): [ 0.00200501 -0.00068563 -0.00237764]\n",
      "Neuron 0 (memory): syn=0.0001, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0022, new_state=-0.0019, dstate=0.0016\n",
      "Neuron 1 (inhibition): original syn=-0.0012, inhibited syn=0.0006, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0014, new_state=0.0013, dstate=-0.0006\n",
      "Neuron 2 (amplification): original syn=0.0005, amplified syn=0.0008, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0035, new_state=0.0031, dstate=-0.0017\n",
      "==================================================\n",
      "Time Step: 16\n",
      "Character: 'f' (ord=102, normalized=0.4000)\n",
      "dt: 0.2145s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0003\n",
      "Input to Hebbian layer: [0.4, 0.2, -0.019999999999999997]\n",
      "Hebbian outputs (to neurons): [ 0.00632031 -0.00221492 -0.00382456]\n",
      "Neuron 0 (memory): syn=-0.0016, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0019, new_state=-0.0020, dstate=-0.0004\n",
      "Neuron 1 (inhibition): original syn=-0.0025, inhibited syn=0.0012, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0013, new_state=0.0013, dstate=0.0002\n",
      "Neuron 2 (amplification): original syn=0.0018, amplified syn=0.0027, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0031, new_state=0.0032, dstate=0.0005\n",
      "==================================================\n",
      "Time Step: 17\n",
      "Character: 'o' (ord=111, normalized=0.4353)\n",
      "dt: 0.2262s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0003\n",
      "Input to Hebbian layer: [0.43529411764705883, 0.21764705882352942, -0.012941176470588234]\n",
      "Hebbian outputs (to neurons): [ 0.00690104 -0.00239169 -0.00401828]\n",
      "Neuron 0 (memory): syn=-0.0018, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0020, new_state=-0.0021, dstate=-0.0005\n",
      "Neuron 1 (inhibition): original syn=-0.0026, inhibited syn=0.0013, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0013, new_state=0.0014, dstate=0.0002\n",
      "Neuron 2 (amplification): original syn=0.0020, amplified syn=0.0030, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0032, new_state=0.0034, dstate=0.0007\n",
      "==================================================\n",
      "Time Step: 18\n",
      "Character: 'x' (ord=120, normalized=0.4706)\n",
      "dt: 0.2165s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0003\n",
      "Input to Hebbian layer: [0.47058823529411764, 0.23529411764705882, -0.005882352941176472]\n",
      "Hebbian outputs (to neurons): [ 0.00746017 -0.00258193 -0.00420228]\n",
      "Neuron 0 (memory): syn=-0.0020, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0021, new_state=-0.0022, dstate=-0.0006\n",
      "Neuron 1 (inhibition): original syn=-0.0028, inhibited syn=0.0014, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0014, new_state=0.0014, dstate=0.0003\n",
      "Neuron 2 (amplification): original syn=0.0022, amplified syn=0.0033, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0034, new_state=0.0036, dstate=0.0008\n",
      "==================================================\n",
      "Time Step: 19\n",
      "Character: ' ' (ord=32, normalized=0.1255)\n",
      "dt: 0.2217s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0004\n",
      "Input to Hebbian layer: [0.12549019607843137, 0.06274509803921569, -0.07490196078431373]\n",
      "Hebbian outputs (to neurons): [ 0.00200607 -0.00068189 -0.00237512]\n",
      "Neuron 0 (memory): syn=0.0001, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0022, new_state=-0.0019, dstate=0.0016\n",
      "Neuron 1 (inhibition): original syn=-0.0012, inhibited syn=0.0006, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0014, new_state=0.0013, dstate=-0.0006\n",
      "Neuron 2 (amplification): original syn=0.0005, amplified syn=0.0008, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0036, new_state=0.0032, dstate=-0.0017\n",
      "==================================================\n",
      "Time Step: 20\n",
      "Character: 'j' (ord=106, normalized=0.4157)\n",
      "dt: 0.2210s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0003\n",
      "Input to Hebbian layer: [0.41568627450980394, 0.20784313725490197, -0.01686274509803921]\n",
      "Hebbian outputs (to neurons): [ 0.00656083 -0.0022962  -0.00389522]\n",
      "Neuron 0 (memory): syn=-0.0017, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0019, new_state=-0.0020, dstate=-0.0005\n",
      "Neuron 1 (inhibition): original syn=-0.0025, inhibited syn=0.0013, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0013, new_state=0.0013, dstate=0.0002\n",
      "Neuron 2 (amplification): original syn=0.0019, amplified syn=0.0028, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0032, new_state=0.0033, dstate=0.0006\n",
      "==================================================\n",
      "Time Step: 21\n",
      "Character: 'u' (ord=117, normalized=0.4588)\n",
      "dt: 0.2196s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0003\n",
      "Input to Hebbian layer: [0.4588235294117647, 0.22941176470588234, -0.008235294117647063]\n",
      "Hebbian outputs (to neurons): [ 0.007266   -0.00251468 -0.00412947]\n",
      "Neuron 0 (memory): syn=-0.0020, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0020, new_state=-0.0021, dstate=-0.0006\n",
      "Neuron 1 (inhibition): original syn=-0.0028, inhibited syn=0.0014, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0013, new_state=0.0014, dstate=0.0003\n",
      "Neuron 2 (amplification): original syn=0.0021, amplified syn=0.0032, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0033, new_state=0.0035, dstate=0.0008\n",
      "==================================================\n",
      "Time Step: 22\n",
      "Character: 'm' (ord=109, normalized=0.4275)\n",
      "dt: 0.2164s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0003\n",
      "Input to Hebbian layer: [0.42745098039215684, 0.21372549019607842, -0.014509803921568632]\n",
      "Hebbian outputs (to neurons): [ 0.00677204 -0.00233809 -0.0039612 ]\n",
      "Neuron 0 (memory): syn=-0.0017, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0021, new_state=-0.0022, dstate=-0.0003\n",
      "Neuron 1 (inhibition): original syn=-0.0026, inhibited syn=0.0013, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0014, new_state=0.0014, dstate=0.0001\n",
      "Neuron 2 (amplification): original syn=0.0020, amplified syn=0.0029, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0035, new_state=0.0036, dstate=0.0005\n",
      "==================================================\n",
      "Time Step: 23\n",
      "Character: 'p' (ord=112, normalized=0.4392)\n",
      "dt: 0.2204s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0004\n",
      "Input to Hebbian layer: [0.4392156862745098, 0.2196078431372549, -0.012156862745098041]\n",
      "Hebbian outputs (to neurons): [ 0.00695249 -0.00240283 -0.00401779]\n",
      "Neuron 0 (memory): syn=-0.0018, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0022, new_state=-0.0023, dstate=-0.0004\n",
      "Neuron 1 (inhibition): original syn=-0.0027, inhibited syn=0.0013, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0014, new_state=0.0015, dstate=0.0001\n",
      "Neuron 2 (amplification): original syn=0.0020, amplified syn=0.0030, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0036, new_state=0.0037, dstate=0.0005\n",
      "==================================================\n",
      "Time Step: 24\n",
      "Character: 's' (ord=115, normalized=0.4510)\n",
      "dt: 0.2178s, Dopamine Level: 0.0000, tau_mod: 1.0000\n",
      "Feedback applied to first neuron: 0.0004\n",
      "Input to Hebbian layer: [0.45098039215686275, 0.22549019607843138, -0.00980392156862745]\n",
      "Hebbian outputs (to neurons): [ 0.00713643 -0.0024649  -0.00407546]\n",
      "Neuron 0 (memory): syn=-0.0019, tau=1.5000\n",
      "Neuron 0 (memory): old_state=-0.0023, new_state=-0.0023, dstate=-0.0004\n",
      "Neuron 1 (inhibition): original syn=-0.0027, inhibited syn=0.0014, tau=1.2000\n",
      "Neuron 1 (inhibition): old_state=0.0015, new_state=0.0015, dstate=0.0001\n",
      "Neuron 2 (amplification): original syn=0.0021, amplified syn=0.0031, tau=1.4000\n",
      "Neuron 2 (amplification): old_state=0.0037, new_state=0.0038, dstate=0.0005\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 121\u001b[0m\n\u001b[1;32m    118\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m    120\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose(fig)\n\u001b[0;32m--> 121\u001b[0m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m network\u001b[38;5;241m.\u001b[39mplot_hebb_weights()\n\u001b[1;32m    123\u001b[0m network\u001b[38;5;241m.\u001b[39mplot_recurrent_hebb_weights()\n",
      "Cell \u001b[0;32mIn[2], line 62\u001b[0m, in \u001b[0;36mAttentiveNetworkWithHebbAndRoles.plot_results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# States\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_neurons):\n\u001b[0;32m---> 62\u001b[0m     ax1\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_steps, \u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeuron \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons[i]\u001b[38;5;241m.\u001b[39mrole\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     63\u001b[0m ax1\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeuron States\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     64\u001b[0m ax1\u001b[38;5;241m.\u001b[39mlegend()\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 3 (Modified)\n",
    "# Example input stream: a sequence of characters\n",
    "input_stream = list(\"The quick brown fox jumps\")\n",
    "\n",
    "# Initialize the updated network with roles, dopamine, and recurrence as before\n",
    "network = AttentiveNetworkWithHebbAndRoles(\n",
    "    roles=[\"memory\", \"inhibition\", \"amplification\"],\n",
    "    input_size=3, \n",
    "    eta=0.005,\n",
    "    dopamine_increase_rate=0.05,\n",
    "    dopamine_decay_rate=0.01,\n",
    "    recurrent_eta=0.002 # Added recurrent_eta\n",
    ")\n",
    "\n",
    "def char_to_inputs(ch):\n",
    "    val = ord(ch) / 255.0\n",
    "    return [val, val/2, (val-0.5)*0.2]\n",
    "\n",
    "states_over_time = []\n",
    "time_steps = []\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "plt.ioff()\n",
    "\n",
    "last_time = time.time()\n",
    "\n",
    "for t, ch in enumerate(input_stream):\n",
    "    inp = char_to_inputs(ch)\n",
    "    current_time = time.time()\n",
    "    dt = current_time - last_time\n",
    "    last_time = current_time\n",
    "\n",
    "    # Before update, gather some internal info:\n",
    "    # We'll temporarily modify AttentiveNetworkWithHebbAndRoles to store intermediate vars for logging:\n",
    "    # (If you prefer not to modify the class, you can subclass or just print after calling update_step.)\n",
    "    # For this example, let's assume we add a method to return debug info after calling update_step.\n",
    "\n",
    "    # Manually replicate some logic from update_step to get intermediate steps for logging:\n",
    "    input_strength = np.mean(inp)\n",
    "    network.dopamine.update(input_strength)\n",
    "    tau_mod = network.dopamine.get_dopamine_modulation()\n",
    "    hebb_outputs = network.hebb.forward(np.array(inp))\n",
    "    feedback = network.neurons[-1].state * 0.1 if network.neurons else 0.0\n",
    "\n",
    "    # Print debugging info for this time slice\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Time Step: {t}\")\n",
    "    print(f\"Character: '{ch}' (ord={ord(ch)}, normalized={ord(ch)/255.0:.4f})\")\n",
    "    print(f\"dt: {dt:.4f}s, Dopamine Level: {network.dopamine.dopamine_level:.4f}, tau_mod: {tau_mod:.4f}\")\n",
    "    print(f\"Feedback applied to first neuron: {feedback:.4f}\")\n",
    "    print(\"Input to Hebbian layer:\", inp)\n",
    "    print(\"Hebbian outputs (to neurons):\", hebb_outputs)\n",
    "\n",
    "    # Now do the actual update_step as before\n",
    "    states = []\n",
    "    pre_activities = np.array(inp)\n",
    "    post_activities_list = []\n",
    "    # We'll replicate the internal loop of update_step for logging details:\n",
    "    for i, neuron in enumerate(network.neurons):\n",
    "        fb = feedback if i == 0 else 0.0\n",
    "\n",
    "        # Detailed logging for each neuron\n",
    "        synaptic_input = np.dot(neuron.weights, hebb_outputs) + fb\n",
    "        if neuron.role == \"memory\":\n",
    "            effective_tau = neuron.base_tau * tau_mod * 1.5\n",
    "            # memory role logic\n",
    "            dstate = (-neuron.state / effective_tau) + synaptic_input\n",
    "        elif neuron.role == \"inhibition\":\n",
    "            inh_syn = -synaptic_input * 0.5\n",
    "            effective_tau = neuron.base_tau * tau_mod\n",
    "            dstate = (-neuron.state / effective_tau) + inh_syn\n",
    "            # For logging, show transform:\n",
    "            print(f\"Neuron {i} ({neuron.role}): original syn={synaptic_input:.4f}, inhibited syn={inh_syn:.4f}, tau={effective_tau:.4f}\")\n",
    "        elif neuron.role == \"amplification\":\n",
    "            amp_syn = synaptic_input * 1.5\n",
    "            effective_tau = neuron.base_tau * tau_mod\n",
    "            dstate = (-neuron.state / effective_tau) + amp_syn\n",
    "            print(f\"Neuron {i} ({neuron.role}): original syn={synaptic_input:.4f}, amplified syn={amp_syn:.4f}, tau={effective_tau:.4f}\")\n",
    "        else:\n",
    "            effective_tau = neuron.base_tau * tau_mod\n",
    "            dstate = (-neuron.state / effective_tau) + synaptic_input\n",
    "        \n",
    "        # If memory role, also log details\n",
    "        if neuron.role == \"memory\":\n",
    "            print(f\"Neuron {i} ({neuron.role}): syn={synaptic_input:.4f}, tau={effective_tau:.4f}\")\n",
    "\n",
    "        # Update neuron state\n",
    "        old_state = neuron.state\n",
    "        neuron.state += dstate * dt\n",
    "        states.append(neuron.state)\n",
    "        post_activities_list.append(neuron.state)\n",
    "\n",
    "        print(f\"Neuron {i} ({neuron.role}): old_state={old_state:.4f}, new_state={neuron.state:.4f}, dstate={dstate:.4f}\")\n",
    "\n",
    "    # After updating neurons, perform Hebbian update\n",
    "    post_activities = np.array(post_activities_list)\n",
    "    network.hebb.hebbian_update(pre_activities, post_activities)\n",
    "    \n",
    "    # Record states\n",
    "    states_over_time.append(states)\n",
    "    time_steps.append(t)\n",
    "\n",
    "    # Plotting as before\n",
    "    #clear_output(wait=True)\n",
    "    \n",
    "    ax.clear()\n",
    "    states_array = np.array(states_over_time)\n",
    "    for i, neuron in enumerate(network.neurons):\n",
    "        ax.plot(time_steps, states_array[:, i], label=f'Neuron {i} ({neuron.role})')\n",
    "    \n",
    "    ax.set_title(f'Neuron States Over Time (last input: \"{ch}\")')\n",
    "    ax.set_xlabel('Time Step')\n",
    "    ax.set_ylabel('State')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    \n",
    "    #display(fig)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "plt.close(fig)\n",
    "network.plot_results()\n",
    "network.plot_hebb_weights()\n",
    "network.plot_recurrent_hebb_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
