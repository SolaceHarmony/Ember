{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "# --- Previously Tested Code (Stable Baseline) ---\n",
    "# For example, fractal connectivity and LTC neurons, Hebbian, etc.\n",
    "\n",
    "def generate_fractal_connectivity(num_neurons, levels=3, block_connect_prob=0.7, sparse_prob=0.02):\n",
    "    W = np.zeros((num_neurons, num_neurons))\n",
    "    def fractal_block(start, end, level):\n",
    "        size = end - start\n",
    "        if level == 0:\n",
    "            block = (np.random.rand(size,size)<block_connect_prob).astype(float)\n",
    "            np.fill_diagonal(block,0.0)\n",
    "            W[start:end,start:end]=block\n",
    "        else:\n",
    "            segments=2\n",
    "            segment_size=size//segments\n",
    "            for i in range(segments):\n",
    "                s_start=start+i*segment_size\n",
    "                s_end=s_start+segment_size\n",
    "                fractal_block(s_start, s_end, level-1)\n",
    "    fractal_block(0, num_neurons, levels-1)\n",
    "\n",
    "    background=(np.random.rand(num_neurons,num_neurons)<sparse_prob).astype(float)\n",
    "    np.fill_diagonal(background,0.0)\n",
    "    W = np.maximum(W,background)\n",
    "    col_sums = W.sum(axis=0)+1e-8\n",
    "    W/=col_sums\n",
    "    return W\n",
    "\n",
    "def hebbian_update(W, pre, post, lr=0.001):\n",
    "    delta=lr*np.outer(post,pre)\n",
    "    W+=delta\n",
    "    col_sums=W.sum(axis=0)+1e-8\n",
    "    W/=col_sums\n",
    "    return W\n",
    "\n",
    "class LTCNeuron:\n",
    "    def __init__(self,tau=1.0):\n",
    "        self.state=0.0\n",
    "        self.tau=tau\n",
    "    def update(self,input_signal,dt=0.1):\n",
    "        dstate=(1/self.tau)*(input_signal - self.state)*dt\n",
    "        self.state+=dstate\n",
    "        return self.state\n",
    "    \n",
    "class ObservableLTCNeuron(LTCNeuron):\n",
    "    def __init__(self, tau=1.0):\n",
    "        super().__init__(tau=tau)\n",
    "        self.dopamine = 0.5\n",
    "        self.serotonin = 1.0\n",
    "        self.norepinephrine = 0.3\n",
    "        self.state_history = []\n",
    "\n",
    "    def compute_reward(self):\n",
    "        return np.random.choice([0, 1], p=[0.9, 0.1])\n",
    "\n",
    "    def update(self, input_signal, dt=0.1):\n",
    "        dh = (1/self.tau)*(input_signal - self.state)*dt\n",
    "        self.state += dh\n",
    "\n",
    "        # Neuromodulator placeholder updates\n",
    "        d_dopamine = (-self.dopamine + self.compute_reward()) / 0.1\n",
    "        d_serotonin = (-self.serotonin + 1.0)/1.0\n",
    "        d_norepinephrine = (-self.norepinephrine + abs(dh))/0.2\n",
    "\n",
    "        self.dopamine += d_dopamine*dt\n",
    "        self.serotonin += d_serotonin*dt\n",
    "        self.norepinephrine += d_norepinephrine*dt\n",
    "\n",
    "        self.state_history.append({\n",
    "            'state': self.state,\n",
    "            'dopamine': self.dopamine,\n",
    "            'serotonin': self.serotonin,\n",
    "            'norepinephrine': self.norepinephrine\n",
    "        })\n",
    "        return self.state\n",
    "# --- Add New Classes without integrating them yet ---\n",
    "class MotorNeuronWithCuriosity:\n",
    "    def __init__(self, threshold_time=1.0, decay_rate=0.1, focus_strength=1.0, curiosity_weight=0.5):\n",
    "        self.threshold_time=threshold_time\n",
    "        self.decay_rate=decay_rate\n",
    "        self.focus_strength=focus_strength\n",
    "        self.curiosity_weight=curiosity_weight\n",
    "        self.focus_history=[]\n",
    "        self.attention_weights={}\n",
    "        self.novelty_scores={}\n",
    "\n",
    "    def initialize_tokens(self,tokens):\n",
    "        for token in tokens:\n",
    "            self.attention_weights[token]=self.focus_strength\n",
    "            self.novelty_scores[token]=0.0\n",
    "\n",
    "    def update_focus(self,token_id,focus_time):\n",
    "        if focus_time>self.threshold_time:\n",
    "            self.attention_weights[token_id]*=(1-self.decay_rate)\n",
    "        else:\n",
    "            self.attention_weights[token_id]*=(1+self.decay_rate)\n",
    "        self.focus_history.append((token_id,focus_time))\n",
    "        self._normalize_weights()\n",
    "\n",
    "    def update_curiosity(self,token_id,prediction_error):\n",
    "        self.novelty_scores[token_id]+=prediction_error\n",
    "\n",
    "    def decide_next_focus(self,candidates,temperature=0.5):\n",
    "        engagement_scores=[]\n",
    "        for token in candidates:\n",
    "            attention_weight=self.attention_weights.get(token,1.0)\n",
    "            curiosity_score=self.novelty_scores.get(token,0.0)\n",
    "            engagement_score=(1 - attention_weight) + self.curiosity_weight*curiosity_score\n",
    "            engagement_scores.append(engagement_score)\n",
    "        probabilities=np.exp(np.array(engagement_scores)/temperature)\n",
    "        probabilities/=probabilities.sum()\n",
    "        return np.random.choice(candidates,p=probabilities)\n",
    "\n",
    "    def _normalize_weights(self):\n",
    "        total_weight=sum(self.attention_weights.values())\n",
    "        if total_weight>0:\n",
    "            for token_id in self.attention_weights:\n",
    "                self.attention_weights[token_id]/=total_weight\n",
    "\n",
    "class CausalAttention:\n",
    "    def __init__(self,decay_rate=0.1,novelty_threshold=0.3,memory_length=100):\n",
    "        self.states={}\n",
    "        self.history=[]\n",
    "        self.decay_rate=decay_rate\n",
    "        self.novelty_threshold=novelty_threshold\n",
    "        self.memory_length=memory_length\n",
    "\n",
    "    def update(self,neuron_id,prediction_error,current_state,target_state):\n",
    "        temporal_decay = np.exp(-self.decay_rate*len(self.history))\n",
    "        causal_weight = 1 - min(abs(prediction_error),1.0)\n",
    "        novelty=abs(target_state - current_state)\n",
    "        novelty_weight=novelty if novelty>self.novelty_threshold else 0.0\n",
    "        attention_value=(temporal_decay+causal_weight+novelty_weight)/3.0\n",
    "        self.history.append((neuron_id,attention_value))\n",
    "        if len(self.history)>self.memory_length:\n",
    "            self.history.pop(0)\n",
    "        return attention_value\n",
    "\n",
    "class HebbianLayer:\n",
    "    def __init__(self,input_size,output_size,eta=0.01):\n",
    "        self.weights=np.random.randn(output_size,input_size)*0.01\n",
    "        self.eta=eta\n",
    "    def forward(self,inputs):\n",
    "        return self.weights@inputs\n",
    "    def hebbian_update(self,inputs,outputs):\n",
    "        delta_w=self.eta*np.outer(outputs,inputs)\n",
    "        self.weights+=delta_w\n",
    "        col_sums=self.weights.sum(axis=0)+1e-8\n",
    "        self.weights=self.weights/col_sums\n",
    "\n",
    "class Neuromodulators:\n",
    "    def __init__(self,dopamine=0.5,serotonin=1.0,norepinephrine=0.3):\n",
    "        self.dopamine=dopamine\n",
    "        self.serotonin=serotonin\n",
    "        self.norepinephrine=norepinephrine\n",
    "    def modulate_parameters(self,base_lr,base_noise_std,base_dt):\n",
    "        dt=base_dt*(1.0+self.dopamine*0.5)\n",
    "        noise_std=base_noise_std*(1.0-self.serotonin*0.5)\n",
    "        lr=base_lr*(1.0+self.norepinephrine*0.2)\n",
    "        return lr, noise_std, dt\n",
    "\n",
    "class TemporalBindingPool:\n",
    "    def __init__(self):\n",
    "        self.bindings={}\n",
    "        self.pattern_strengths={}\n",
    "    def bind(self,time,patterns):\n",
    "        self.bindings[time]=set(patterns)\n",
    "        for p1 in patterns:\n",
    "            for p2 in patterns:\n",
    "                if p1!=p2:\n",
    "                    key=tuple(sorted([p1,p2]))\n",
    "                    self.pattern_strengths[key]=self.pattern_strengths.get(key,0)+1\n",
    "    def get_stable_patterns(self,threshold=3):\n",
    "        return {k:v for k,v in self.pattern_strengths.items() if v>=threshold}\n",
    "\n",
    "class PatternMetrics:\n",
    "    def __init__(self):\n",
    "        self.pattern_counts={}\n",
    "        self.transition_counts={}   \n",
    "        self.mutual_info_cache={}\n",
    "    def update(self,pattern,next_pattern=None):\n",
    "        self.pattern_counts[pattern]=self.pattern_counts.get(pattern,0)+1\n",
    "        if next_pattern:\n",
    "            key=(pattern,next_pattern)\n",
    "            self.transition_counts[key]=self.transition_counts.get(key,0)+1\n",
    "    def get_mutual_information(self,pattern1,pattern2):\n",
    "        key=tuple(sorted([pattern1,pattern2]))\n",
    "        if key in self.mutual_info_cache:\n",
    "            return self.mutual_info_cache[key]\n",
    "        p_x=self.pattern_counts.get(pattern1,0)/max(1,sum(self.pattern_counts.values()))\n",
    "        p_y=self.pattern_counts.get(pattern2,0)/max(1,sum(self.pattern_counts.values()))\n",
    "        p_xy=self.transition_counts.get((pattern1,pattern2),0)/max(1,sum(self.transition_counts.values()))\n",
    "        if p_xy>0:\n",
    "            mi=p_xy*math.log((p_xy/(p_x*p_y+1e-9))+1e-9)\n",
    "            self.mutual_info_cache[key]=mi\n",
    "            return mi\n",
    "        return 0\n",
    "\n",
    "# We can now rely on these classes as building blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure W is defined here, before run_phase is defined or called\n",
    "num_neurons = 200\n",
    "W = generate_fractal_connectivity(num_neurons, levels=3, block_connect_prob=0.7, sparse_prob=0.02)\n",
    "\n",
    "# Initialize neurons, motor_neuron, etc.\n",
    "neurons = [ObservableLTCNeuron(tau=1.0) for _ in range(num_neurons)]\n",
    "tokens = ['a', 'b', 'c']\n",
    "motor_neuron = MotorNeuronWithCuriosity(threshold_time=0.8, decay_rate=0.1, focus_strength=1.0, curiosity_weight=0.5)\n",
    "motor_neuron.initialize_tokens(tokens)\n",
    "\n",
    "noise_std = 0.05\n",
    "learning_rate = 0.001\n",
    "dt = 0.1\n",
    "num_steps = 600\n",
    "\n",
    "def generate_token_harmonic(token, freq_map={'a':5,'b':7,'c':3}):\n",
    "    freq = freq_map[token]\n",
    "    t = np.linspace(0,1,10)\n",
    "    signal = np.sin(2*np.pi*freq*t)\n",
    "    return signal[0]\n",
    "\n",
    "input_neurons = np.arange(0,10)\n",
    "activity_record = []\n",
    "\n",
    "def run_phase(token, steps):\n",
    "    global W\n",
    "    for _ in range(steps):\n",
    "        for tk in tokens:\n",
    "            if tk == token:\n",
    "                motor_neuron.attention_weights[tk] = 1.0\n",
    "            else:\n",
    "                motor_neuron.attention_weights[tk] = 0.01\n",
    "        motor_neuron._normalize_weights()\n",
    "\n",
    "        current_token = motor_neuron.decide_next_focus(tokens, temperature=0.5)\n",
    "        current_input_val = generate_token_harmonic(current_token)\n",
    "\n",
    "        external_input = np.zeros(num_neurons)\n",
    "        external_input[input_neurons] = current_input_val\n",
    "        external_input += np.random.randn(num_neurons)*noise_std\n",
    "\n",
    "        old_states = np.array([n.state for n in neurons])\n",
    "        # W is defined above, so now it should be accessible\n",
    "        input_vector = W.dot(old_states) + external_input\n",
    "\n",
    "        new_states=[]\n",
    "        for i, neuron in enumerate(neurons):\n",
    "            s = neuron.update(input_vector[i], dt=dt)\n",
    "            new_states.append(s)\n",
    "\n",
    "        # Hebbian update\n",
    "        W = hebbian_update(W, old_states, new_states, lr=learning_rate)\n",
    "        activity_record.append(new_states.copy())\n",
    "\n",
    "        # Simulate focus_time and prediction_error\n",
    "        focus_time = np.random.uniform(0.5,1.5) \n",
    "        prediction_error = np.random.uniform(0.0,0.3)\n",
    "        motor_neuron.update_focus(current_token, focus_time)\n",
    "        motor_neuron.update_curiosity(current_token, prediction_error)\n",
    "\n",
    "# PHASE 1\n",
    "run_phase('a', 200)\n",
    "# PHASE 2\n",
    "run_phase('b', 200)\n",
    "# PHASE 3\n",
    "run_phase('c', 200)\n",
    "\n",
    "activity_record = np.array(activity_record)\n",
    "print(\"Staged input simulation complete. Now run Cell 3 to visualize results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Average Activity Over Time (Baseline with Observability)\")\n",
    "plt.plot(activity_record.mean(axis=1), label='Mean Activity')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Mean State')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Sample Neurons Activity (Baseline with Observability)\")\n",
    "for i in range(10):\n",
    "    plt.plot(activity_record[:, i], label=f'Neuron {i}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('State')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
