{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class WordBoundaryDetector:\n",
    "    def __init__(self, threshold=0.1, attention_decay=0.1):\n",
    "        self.threshold = threshold\n",
    "        self.attention_decay = attention_decay\n",
    "        self.attention_map = {}\n",
    "        self.boundaries = []\n",
    "        \n",
    "    def process_stream(self, text_stream):\n",
    "        # Initialize attention for each character\n",
    "        for char in text_stream:\n",
    "            if char not in self.attention_map:\n",
    "                self.attention_map[char] = 1.0  # Start with full attention\n",
    "\n",
    "        # Process character stream\n",
    "        for i, char in enumerate(text_stream):\n",
    "            # Update attention based on character type\n",
    "            if char == ' ':\n",
    "                # Spaces get boring quickly\n",
    "                self.attention_map[char] *= (1 - self.attention_decay)\n",
    "            elif char in '.!?,;':\n",
    "                # Punctuation maintains moderate attention as flavor\n",
    "                self.attention_map[char] = max(0.5, self.attention_map[char])\n",
    "            else:\n",
    "                # Letters maintain attention based on novelty\n",
    "                prev_char = text_stream[i-1] if i > 0 else None\n",
    "                next_char = text_stream[i+1] if i < len(text_stream)-1 else None\n",
    "                \n",
    "                # Compute energy gradient\n",
    "                energy = self.compute_energy(char, prev_char, next_char)\n",
    "                probability = self.boltzmann(energy)\n",
    "                \n",
    "                # Update attention based on probability\n",
    "                self.attention_map[char] *= probability\n",
    "\n",
    "        # Find word boundaries using attention gradients\n",
    "        return self.detect_boundaries(text_stream)\n",
    "\n",
    "    def compute_energy(self, char, prev_char, next_char):\n",
    "        # Energy is lower (more favorable) for:\n",
    "        # - Letters that form common patterns\n",
    "        # - Characters that maintain meaning\n",
    "        base_energy = -self.attention_map[char]\n",
    "        \n",
    "        # Adjust for context\n",
    "        if prev_char:\n",
    "            base_energy -= 0.5 * self.attention_map[prev_char]\n",
    "        if next_char:\n",
    "            base_energy -= 0.5 * self.attention_map[next_char]\n",
    "            \n",
    "        return base_energy\n",
    "\n",
    "    def boltzmann(self, energy, temperature=0.5):\n",
    "        return math.exp(-energy / temperature)\n",
    "\n",
    "    def detect_boundaries(self, text_stream):\n",
    "        words = []\n",
    "        current_word = []\n",
    "        attention_threshold = sum(self.attention_map.values()) / len(self.attention_map)\n",
    "\n",
    "        for i, char in enumerate(text_stream):\n",
    "            if self.attention_map[char] > attention_threshold:\n",
    "                # High attention character - part of a word\n",
    "                current_word.append(char)\n",
    "            else:\n",
    "                # Low attention (like space) - potential boundary\n",
    "                if current_word:\n",
    "                    words.append(''.join(current_word))\n",
    "                    current_word = []\n",
    "                    \n",
    "                if char not in ' \\n\\t':  # Keep punctuation as separate tokens\n",
    "                    words.append(char)\n",
    "\n",
    "        if current_word:  # Don't forget last word\n",
    "            words.append(''.join(current_word))\n",
    "\n",
    "        return words\n",
    "\n",
    "    def visualize_attention(self):\n",
    "        # Return attention values for visualization\n",
    "        return self.attention_map\n",
    "    \n",
    "class LTCNeuron:\n",
    "    def __init__(self, tau=1.0, context_size=3):\n",
    "        self.state = 0\n",
    "        self.tau = tau\n",
    "        self.context_window = []\n",
    "        self.context_size = context_size\n",
    "        self.prediction_history = []\n",
    "        self.consolidated_patterns = {}  # pattern -> frequency\n",
    "        self.stable_states = set()  # Consolidated memory states\n",
    "\n",
    "    def update(self, input_signal, context, dt):\n",
    "        # Update context window\n",
    "        self.context_window.append(input_signal)\n",
    "        if len(self.context_window) > self.context_size:\n",
    "            self.context_window.pop(0)\n",
    "\n",
    "        # Predict based on context history\n",
    "        prediction = self.predict_next(context)\n",
    "        self.prediction_history.append((prediction, input_signal))\n",
    "\n",
    "        # LTC dynamics with context influence\n",
    "        context_influence = sum(c * 0.1 for c in context)\n",
    "        self.state += (1/self.tau) * (input_signal + context_influence - self.state) * dt\n",
    "\n",
    "        # Check for pattern consolidation\n",
    "        self.consolidate_if_stable()\n",
    "\n",
    "        return self.state, prediction\n",
    "\n",
    "    def predict_next(self, context):\n",
    "        if not self.prediction_history:\n",
    "            return 0\n",
    "\n",
    "        # Look for similar contexts in history\n",
    "        similar_contexts = []\n",
    "        for pred, actual in self.prediction_history[-10:]:  # Last 10 predictions\n",
    "            if self.context_matches(context):\n",
    "                similar_contexts.append(actual)\n",
    "\n",
    "        return sum(similar_contexts) / len(similar_contexts) if similar_contexts else 0\n",
    "\n",
    "    def consolidate_if_stable(self, stability_threshold=3):\n",
    "        if len(self.context_window) < self.context_size:\n",
    "            return\n",
    "\n",
    "        pattern = tuple(self.context_window)\n",
    "        self.consolidated_patterns[pattern] = self.consolidated_patterns.get(pattern, 0) + 1\n",
    "\n",
    "        if self.consolidated_patterns[pattern] >= stability_threshold:\n",
    "            self.stable_states.add(pattern)\n",
    "\n",
    "class TemporalBindingPool:\n",
    "    def __init__(self):\n",
    "        self.bindings = {}  # time -> set of active patterns\n",
    "        self.pattern_strengths = {}  # pattern -> strength\n",
    "\n",
    "    def bind(self, time, patterns):\n",
    "        self.bindings[time] = set(patterns)\n",
    "        \n",
    "        # Strengthen co-occurring patterns\n",
    "        for p1 in patterns:\n",
    "            for p2 in patterns:\n",
    "                if p1 != p2:\n",
    "                    key = (min(p1, p2), max(p1, p2))\n",
    "                    self.pattern_strengths[key] = self.pattern_strengths.get(key, 0) + 1\n",
    "\n",
    "    def get_stable_patterns(self, threshold=3):\n",
    "        return {k: v for k, v in self.pattern_strengths.items() if v >= threshold}\n",
    "\n",
    "class EnhancedCuriosityDrivenSegmenter:\n",
    "    def __init__(self, temperature=0.5):\n",
    "        self.neurons = {}\n",
    "        self.temporal_binding = TemporalBindingPool()\n",
    "        self.temperature = temperature\n",
    "        self.time_index = 0\n",
    "        self.memory_cache = {}  # Consolidated patterns cache\n",
    "\n",
    "    def process_sequence(self, text, dt=0.1):\n",
    "        context_window = []\n",
    "        consolidated_segments = []\n",
    "\n",
    "        for i, char in enumerate(text):\n",
    "            # Get context\n",
    "            context = text[max(0, i-3):i]\n",
    "            context_window.append(char)\n",
    "            \n",
    "            # Process through neurons\n",
    "            neuron = self.get_or_create_neuron(char)\n",
    "            state, prediction = neuron.update(1.0, context, dt)\n",
    "            \n",
    "            # Check for consolidated patterns\n",
    "            if len(context_window) >= 3:\n",
    "                pattern = tuple(context_window[-3:])\n",
    "                if pattern in self.memory_cache:\n",
    "                    # Recognized pattern - use consolidated representation\n",
    "                    consolidated_segments.append(self.memory_cache[pattern])\n",
    "                    context_window = []  # Reset window after recognition\n",
    "                elif self.is_stable_pattern(pattern):\n",
    "                    # New stable pattern found - consolidate to memory\n",
    "                    self.memory_cache[pattern] = ''.join(pattern)\n",
    "                    consolidated_segments.append(self.memory_cache[pattern])\n",
    "                    context_window = []\n",
    "\n",
    "            # Temporal binding\n",
    "            active_patterns = self.get_active_patterns(context_window)\n",
    "            self.temporal_binding.bind(self.time_index, active_patterns)\n",
    "            \n",
    "            self.time_index += 1\n",
    "\n",
    "        return consolidated_segments, self.temporal_binding.get_stable_patterns()\n",
    "\n",
    "    def is_stable_pattern(self, pattern):\n",
    "        # Check if pattern appears in multiple neurons' stable states\n",
    "        stability_count = 0\n",
    "        for neuron in self.neurons.values():\n",
    "            if pattern in neuron.stable_states:\n",
    "                stability_count += 1\n",
    "        return stability_count >= 2  # Pattern is stable if recognized by multiple neurons\n",
    "\n",
    "    def get_active_patterns(self, window):\n",
    "        patterns = set()\n",
    "        for i in range(len(window)):\n",
    "            for j in range(i + 1, len(window) + 1):\n",
    "                patterns.add(tuple(window[i:j]))\n",
    "        return patterns\n",
    "    \n",
    "class PatternMetrics:\n",
    "    def __init__(self):\n",
    "        self.pattern_counts = {}\n",
    "        self.transition_counts = {}\n",
    "        self.mutual_info_cache = {}\n",
    "        \n",
    "    def update(self, pattern, next_pattern=None):\n",
    "        # Update pattern frequencies\n",
    "        self.pattern_counts[pattern] = self.pattern_counts.get(pattern, 0) + 1\n",
    "        \n",
    "        if next_pattern:\n",
    "            # Track transitions for conditional probability\n",
    "            key = (pattern, next_pattern)\n",
    "            self.transition_counts[key] = self.transition_counts.get(key, 0) + 1\n",
    "    \n",
    "    def get_mutual_information(self, pattern1, pattern2):\n",
    "        key = (min(pattern1, pattern2), max(pattern1, pattern2))\n",
    "        if key in self.mutual_info_cache:\n",
    "            return self.mutual_info_cache[key]\n",
    "            \n",
    "        # Calculate mutual information\n",
    "        p_x = self.pattern_counts.get(pattern1, 0) / sum(self.pattern_counts.values())\n",
    "        p_y = self.pattern_counts.get(pattern2, 0) / sum(self.pattern_counts.values())\n",
    "        p_xy = self.transition_counts.get((pattern1, pattern2), 0) / sum(self.transition_counts.values())\n",
    "        \n",
    "        if p_xy > 0:\n",
    "            mi = p_xy * math.log(p_xy / (p_x * p_y))\n",
    "            self.mutual_info_cache[key] = mi\n",
    "            return mi\n",
    "        return 0\n",
    "\n",
    "class HierarchicalPattern:\n",
    "    def __init__(self, base_pattern):\n",
    "        self.base = base_pattern\n",
    "        self.children = []\n",
    "        self.parent = None\n",
    "        self.stability_score = 0\n",
    "        self.level = 0\n",
    "\n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "        child.parent = self\n",
    "        child.level = self.level + 1\n",
    "\n",
    "class AdaptiveTemperature:\n",
    "    def __init__(self, initial_temp=0.5, min_temp=0.1, max_temp=2.0):\n",
    "        self.temperature = initial_temp\n",
    "        self.min_temp = min_temp\n",
    "        self.max_temp = max_temp\n",
    "        self.stability_history = []\n",
    "        self.novelty_rate = 0\n",
    "\n",
    "    def update(self, stability_score, novelty_score):\n",
    "        self.stability_history.append(stability_score)\n",
    "        self.novelty_rate = novelty_score\n",
    "        \n",
    "        if len(self.stability_history) > 10:  # Rolling window\n",
    "            avg_stability = sum(self.stability_history[-10:]) / 10\n",
    "            \n",
    "            # Adjust temperature based on stability and novelty\n",
    "            if avg_stability > 0.8 and self.novelty_rate < 0.2:\n",
    "                # Very stable, increase temperature to encourage exploration\n",
    "                self.temperature = min(self.temperature * 1.1, self.max_temp)\n",
    "            elif avg_stability < 0.3 or self.novelty_rate > 0.8:\n",
    "                # Unstable or too much novelty, decrease temperature\n",
    "                self.temperature = max(self.temperature * 0.9, self.min_temp)\n",
    "\n",
    "class EnhancedCuriosityDrivenSegmenter:\n",
    "    def __init__(self):\n",
    "        self.neurons = {}\n",
    "        self.pattern_metrics = PatternMetrics()\n",
    "        self.hierarchy = {}  # level -> patterns\n",
    "        self.temperature = AdaptiveTemperature()\n",
    "        \n",
    "    def process_sequence(self, text, dt=0.1):\n",
    "        current_level_patterns = []\n",
    "        stability_scores = []\n",
    "        \n",
    "        for level in range(3):  # Process multiple hierarchical levels\n",
    "            patterns = self.process_level(text, level, current_level_patterns)\n",
    "            current_level_patterns = patterns\n",
    "            \n",
    "            # Calculate stability for temperature adjustment\n",
    "            stability = self.calculate_level_stability(patterns)\n",
    "            stability_scores.append(stability)\n",
    "            \n",
    "            # Update temperature based on stability and novelty\n",
    "            novelty = self.calculate_novelty_rate(patterns)\n",
    "            self.temperature.update(stability, novelty)\n",
    "            \n",
    "            # Store patterns in hierarchy\n",
    "            self.hierarchy[level] = patterns\n",
    "            \n",
    "            # If patterns are very stable, use them as base units for next level\n",
    "            if stability > 0.8:\n",
    "                text = self.reconstruct_text_from_patterns(patterns)\n",
    "        \n",
    "        return self.hierarchy\n",
    "\n",
    "    def calculate_level_stability(self, patterns):\n",
    "        if not patterns:\n",
    "            return 0\n",
    "            \n",
    "        stable_count = sum(1 for p in patterns if self.is_stable_pattern(p))\n",
    "        return stable_count / len(patterns)\n",
    "\n",
    "    def calculate_novelty_rate(self, patterns):\n",
    "        known_patterns = set(self.pattern_metrics.pattern_counts.keys())\n",
    "        novel_count = sum(1 for p in patterns if p not in known_patterns)\n",
    "        return novel_count / len(patterns) if patterns else 0\n",
    "\n",
    "    def is_stable_pattern(self, pattern):\n",
    "        # Check multiple stability criteria\n",
    "        frequency_stable = self.pattern_metrics.pattern_counts.get(pattern, 0) > 3\n",
    "        \n",
    "        # Check predictive power using mutual information\n",
    "        if frequency_stable:\n",
    "            mi_scores = []\n",
    "            for other_pattern in self.pattern_metrics.pattern_counts:\n",
    "                if other_pattern != pattern:\n",
    "                    mi = self.pattern_metrics.get_mutual_information(pattern, other_pattern)\n",
    "                    mi_scores.append(mi)\n",
    "            \n",
    "            predictive_power = sum(mi_scores) / len(mi_scores) if mi_scores else 0\n",
    "            return predictive_power > 0.3\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def reconstruct_text_from_patterns(self, patterns):\n",
    "        # Convert stable patterns to tokens for next level\n",
    "        return ' '.join(str(p) for p in patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ed9c5c89e14203a5bcfd0097fab9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=100, description='Iterations:', max=500, min=50, step=50), FloatSlider(vâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive, FloatSlider, HBox, VBox, IntSlider\n",
    "import numpy as np\n",
    "\n",
    "# Synthetic data function:\n",
    "# Simulate attention weights and novelty scores over iterations.\n",
    "def simulate_data(num_iterations=100, temperature=0.5, decay_rate=0.1):\n",
    "    # For demonstration, we'll create a set of patterns:\n",
    "    patterns = [\"A\", \"B\", \"C\", \"D\", \" \"]\n",
    "    # Start attention weights uniformly\n",
    "    attention = {p: [1.0 for _ in range(num_iterations)] for p in patterns}\n",
    "    novelty = {p: [0.0 for _ in range(num_iterations)] for p in patterns}\n",
    "\n",
    "    for t in range(1, num_iterations):\n",
    "        for p in patterns:\n",
    "            # Attention update simulation:\n",
    "            # Decrease attention for space due to boredom:\n",
    "            if p == \" \":\n",
    "                attention[p][t] = attention[p][t-1] * (1 - decay_rate)\n",
    "            else:\n",
    "                # Add some random fluctuation influenced by temperature\n",
    "                fluct = np.random.normal(0, 0.1 * (1/temperature))\n",
    "                attention[p][t] = max(0, attention[p][t-1] + fluct)\n",
    "\n",
    "            # Novelty score simulation:\n",
    "            # If attention spikes or drops significantly, treat it as a novelty sign:\n",
    "            change = abs(attention[p][t] - attention[p][t-1])\n",
    "            novelty[p][t] = max(0, novelty[p][t-1] * 0.9 + change * 0.5)\n",
    "\n",
    "    return patterns, attention, novelty\n",
    "\n",
    "def plot_data(num_iterations=100, temperature=0.5, decay_rate=0.1):\n",
    "    patterns, attention, novelty = simulate_data(num_iterations, temperature, decay_rate)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\n",
    "    \n",
    "    # Plot attention over time\n",
    "    for p in patterns:\n",
    "        ax1.plot(range(num_iterations), attention[p], label=f\"{p!r} Attn\")\n",
    "    ax1.set_title(\"Attention Weights Over Time\")\n",
    "    ax1.set_xlabel(\"Iterations\")\n",
    "    ax1.set_ylabel(\"Attention Weight\")\n",
    "    ax1.grid(True)\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot novelty over time\n",
    "    for p in patterns:\n",
    "        ax2.plot(range(num_iterations), novelty[p], label=f\"{p!r} Novelty\")\n",
    "    ax2.set_title(\"Novelty Scores Over Time\")\n",
    "    ax2.set_xlabel(\"Iterations\")\n",
    "    ax2.set_ylabel(\"Novelty\")\n",
    "    ax2.grid(True)\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create sliders to adjust parameters interactively\n",
    "temp_slider = FloatSlider(value=0.5, min=0.1, max=2.0, step=0.1, description='Temp:')\n",
    "decay_slider = FloatSlider(value=0.1, min=0.01, max=0.5, step=0.01, description='Decay:')\n",
    "iter_slider = IntSlider(value=100, min=50, max=500, step=50, description='Iterations:')\n",
    "\n",
    "ui = VBox([iter_slider, temp_slider, decay_slider])\n",
    "out = interactive(plot_data, num_iterations=iter_slider, temperature=temp_slider, decay_rate=decay_slider)\n",
    "display(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
