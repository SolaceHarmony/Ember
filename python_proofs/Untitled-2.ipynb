{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import mse_loss\n",
    "class HarmonicEmbeddingCache:\n",
    "    def __init__(self, num_tokens=50_000, original_dim=768, num_harmonics=16):\n",
    "        self.num_tokens = num_tokens\n",
    "        self.original_dim = original_dim\n",
    "        self.num_harmonics = num_harmonics\n",
    "        self.cache = defaultdict(dict)  # Cache for harmonic wave parameters\n",
    "        self.embedding_cache = {}       # Cache for original transformer embeddings\n",
    "\n",
    "    def generate_embedding(self, token, model, tokenizer):\n",
    "        # Check if the token embedding is already cached\n",
    "        if token in self.embedding_cache:\n",
    "            return self.embedding_cache[token]\n",
    "\n",
    "        # Generate embedding if it's not in the cache\n",
    "        inputs = tokenizer(token, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].numpy().flatten()  # Use CLS token\n",
    "\n",
    "        # Cache the generated embedding\n",
    "        self.embedding_cache[token] = embedding\n",
    "        return embedding\n",
    "\n",
    "    def encode_embedding(self, embedding):\n",
    "        # Encode embedding into harmonic parameters (amplitudes, frequencies, phases)\n",
    "        assert len(embedding) == self.original_dim, f\"Embedding must have dimension {self.original_dim}\"\n",
    "\n",
    "        amplitudes = np.zeros(self.num_harmonics)\n",
    "        frequencies = np.zeros(self.num_harmonics)\n",
    "        phases = np.zeros(self.num_harmonics)\n",
    "\n",
    "        for h in range(self.num_harmonics):\n",
    "            base_freq = 2.0 ** (h / self.num_harmonics)\n",
    "            window_size = self.original_dim // self.num_harmonics\n",
    "            start, end = h * window_size, min((h + 1) * window_size, self.original_dim)\n",
    "            slice_ = embedding[start:end]\n",
    "\n",
    "            amplitudes[h] = np.mean(slice_)\n",
    "            frequencies[h] = base_freq\n",
    "            phases[h] = np.arctan2(np.sum(slice_), np.sum(np.abs(slice_)))\n",
    "\n",
    "        return {\"amplitudes\": amplitudes, \"frequencies\": frequencies, \"phases\": phases}\n",
    "\n",
    "    def cache_embedding(self, token, embedding):\n",
    "        # Cache the harmonic wave representation of the embedding\n",
    "        wave_params = self.encode_embedding(embedding)\n",
    "        self.cache[token] = wave_params\n",
    "\n",
    "    def get_embedding(self, token):\n",
    "        # Retrieve the harmonic embedding from the cache\n",
    "        if token in self.cache:\n",
    "            return self.decode_embedding(self.cache[token])\n",
    "        return None\n",
    "\n",
    "    def decode_embedding(self, params, t=1.0):\n",
    "        # Reconstruct the original embedding from the harmonic parameters\n",
    "        reconstructed = np.zeros(self.original_dim)\n",
    "        for h in range(self.num_harmonics):\n",
    "            window_size = self.original_dim // self.num_harmonics\n",
    "            start, end = h * window_size, min((h + 1) * window_size, self.original_dim)\n",
    "            wave = params[\"amplitudes\"][h] * np.sin(2 * np.pi * params[\"frequencies\"][h] * t + params[\"phases\"][h])\n",
    "            reconstructed[start:end] = wave\n",
    "\n",
    "        return reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/sxmjmw9n7rlg4cwgzwxxs3nw0000gn/T/ipykernel_16149/134251528.py:74: UserWarning: Using a target size (torch.Size([1, 768])) that is different to the input size (torch.Size([768])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = mse_loss(reconstructed, embedding_tensor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.982714831829071\n",
      "Epoch 2/100, Loss: 0.7348491350809733\n",
      "Epoch 3/100, Loss: 1.004717747370402\n",
      "Epoch 4/100, Loss: 0.8696858882904053\n",
      "Epoch 5/100, Loss: 0.7378442088762919\n",
      "Epoch 6/100, Loss: 0.7100762526194254\n",
      "Epoch 7/100, Loss: 0.7447986801465353\n",
      "Epoch 8/100, Loss: 0.7070588866869608\n",
      "Epoch 9/100, Loss: 0.8395688533782959\n",
      "Epoch 10/100, Loss: 0.65139768520991\n",
      "Epoch 11/100, Loss: 0.9339492718378702\n",
      "Epoch 12/100, Loss: 0.579721599817276\n",
      "Epoch 13/100, Loss: 1.1118858456611633\n",
      "Epoch 14/100, Loss: 0.8231765627861023\n",
      "Epoch 15/100, Loss: 0.6779486437638601\n",
      "Epoch 16/100, Loss: 0.7367963790893555\n",
      "Epoch 17/100, Loss: 0.7691120505332947\n",
      "Epoch 18/100, Loss: 0.7034048835436503\n",
      "Epoch 19/100, Loss: 0.6024362742900848\n",
      "Epoch 20/100, Loss: 0.7102583348751068\n",
      "Epoch 21/100, Loss: 0.808010995388031\n",
      "Epoch 22/100, Loss: 0.8068291743596395\n",
      "Epoch 23/100, Loss: 0.7856375078360239\n",
      "Epoch 24/100, Loss: 0.9902118047078451\n",
      "Epoch 25/100, Loss: 0.7021472851435343\n",
      "Epoch 26/100, Loss: 0.5421416958173116\n",
      "Epoch 27/100, Loss: 0.7619461615880331\n",
      "Epoch 28/100, Loss: 0.9018381039301554\n",
      "Epoch 29/100, Loss: 1.03906253973643\n",
      "Epoch 30/100, Loss: 0.6825614174207052\n",
      "Epoch 31/100, Loss: 0.8101052244504293\n",
      "Epoch 32/100, Loss: 0.6355751951535543\n",
      "Epoch 33/100, Loss: 0.9301438331604004\n",
      "Epoch 34/100, Loss: 0.8355381290117899\n",
      "Epoch 35/100, Loss: 0.7085066437721252\n",
      "Epoch 36/100, Loss: 0.8276583751042684\n",
      "Epoch 37/100, Loss: 0.994001587231954\n",
      "Epoch 38/100, Loss: 0.8346735239028931\n",
      "Epoch 39/100, Loss: 0.7658793131510416\n",
      "Epoch 40/100, Loss: 0.8427337408065796\n",
      "Epoch 41/100, Loss: 0.765574594338735\n",
      "Epoch 42/100, Loss: 0.8866332173347473\n",
      "Epoch 43/100, Loss: 0.7063584129015604\n",
      "Epoch 44/100, Loss: 0.7642504970232645\n",
      "Epoch 45/100, Loss: 0.9902309974034628\n",
      "Epoch 46/100, Loss: 1.045571506023407\n",
      "Epoch 47/100, Loss: 0.8760205308596293\n",
      "Epoch 48/100, Loss: 0.9236304759979248\n",
      "Epoch 49/100, Loss: 0.5790220002333323\n",
      "Epoch 50/100, Loss: 0.8815806905428568\n",
      "Epoch 51/100, Loss: 0.5437060395876566\n",
      "Epoch 52/100, Loss: 0.6752240359783173\n",
      "Epoch 53/100, Loss: 0.6734006702899933\n",
      "Epoch 54/100, Loss: 0.9647345741589864\n",
      "Epoch 55/100, Loss: 0.7325766285260519\n",
      "Epoch 56/100, Loss: 0.564863900343577\n",
      "Epoch 57/100, Loss: 0.6596634785334269\n",
      "Epoch 58/100, Loss: 1.0282416840394337\n",
      "Epoch 59/100, Loss: 0.5740794738133749\n",
      "Epoch 60/100, Loss: 0.6642506917317709\n",
      "Epoch 61/100, Loss: 0.5882808168729147\n",
      "Epoch 62/100, Loss: 1.0539403557777405\n",
      "Epoch 63/100, Loss: 0.6550591985384623\n",
      "Epoch 64/100, Loss: 0.5825206637382507\n",
      "Epoch 65/100, Loss: 0.833143949508667\n",
      "Epoch 66/100, Loss: 0.5859458645184835\n",
      "Epoch 67/100, Loss: 0.5351609488328298\n",
      "Epoch 68/100, Loss: 1.0724686582883198\n",
      "Epoch 69/100, Loss: 0.8587520917256674\n",
      "Epoch 70/100, Loss: 1.0836168726285298\n",
      "Epoch 71/100, Loss: 0.559266189734141\n",
      "Epoch 72/100, Loss: 0.6969112157821655\n",
      "Epoch 73/100, Loss: 0.9850306113560995\n",
      "Epoch 74/100, Loss: 0.7427643338839213\n",
      "Epoch 75/100, Loss: 0.9426267147064209\n",
      "Epoch 76/100, Loss: 1.051368534564972\n",
      "Epoch 77/100, Loss: 0.774470309416453\n",
      "Epoch 78/100, Loss: 0.9367961287498474\n",
      "Epoch 79/100, Loss: 0.9596085151036581\n",
      "Epoch 80/100, Loss: 0.8759010036786398\n",
      "Epoch 81/100, Loss: 0.8444159229596456\n",
      "Epoch 82/100, Loss: 0.6052635908126831\n",
      "Epoch 83/100, Loss: 0.7855280836423238\n",
      "Epoch 84/100, Loss: 1.0386951963106792\n",
      "Epoch 85/100, Loss: 0.815088947614034\n",
      "Epoch 86/100, Loss: 0.951798141002655\n",
      "Epoch 87/100, Loss: 1.0197324355443318\n",
      "Epoch 88/100, Loss: 0.7399229407310486\n",
      "Epoch 89/100, Loss: 0.833860456943512\n",
      "Epoch 90/100, Loss: 0.7106054623921713\n",
      "Epoch 91/100, Loss: 0.6674003998438517\n",
      "Epoch 92/100, Loss: 0.6179144978523254\n",
      "Epoch 93/100, Loss: 0.6073586344718933\n",
      "Epoch 94/100, Loss: 0.7994896769523621\n",
      "Epoch 95/100, Loss: 0.8387706279754639\n",
      "Epoch 96/100, Loss: 0.8485729495684305\n",
      "Epoch 97/100, Loss: 0.6012591123580933\n",
      "Epoch 98/100, Loss: 0.7188242276509603\n",
      "Epoch 99/100, Loss: 0.7706218759218851\n",
      "Epoch 100/100, Loss: 0.7810620268185934\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import mse_loss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Harmonic cache instance\n",
    "harmonic_cache = HarmonicEmbeddingCache(original_dim=768)\n",
    "\n",
    "# Example token list for training\n",
    "tokens = [\"The quick brown fox\", \"AI is transforming the world\", \"Deep learning models are powerful\"]\n",
    "\n",
    "# Initialize Transformer model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Dictionary to store cached embeddings\n",
    "embedding_cache = {}\n",
    "\n",
    "# Learning rate and optimizer setup\n",
    "learning_rate = 0.001\n",
    "\n",
    "def generate_embedding(token):\n",
    "    \"\"\"\n",
    "    Generate or retrieve an embedding for a given token.\n",
    "    Uses a local cache to avoid redundant computations.\n",
    "    \"\"\"\n",
    "    if token in embedding_cache:\n",
    "        return embedding_cache[token]\n",
    "    else:\n",
    "        # Generate and cache embedding\n",
    "        inputs = tokenizer(token, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = model(**inputs)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "        embedding_cache[token] = embedding\n",
    "        return embedding\n",
    "\n",
    "def train_loop(tokens, epochs=10):\n",
    "    \"\"\"\n",
    "    Training loop for using harmonic embeddings from a cached embedding source.\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for token in tokens:\n",
    "            # Get embedding from cache or generate it if not cached\n",
    "            embedding = generate_embedding(token)\n",
    "            \n",
    "            # Convert embedding to PyTorch tensor\n",
    "            embedding_tensor = torch.tensor(embedding, dtype=torch.float32)\n",
    "\n",
    "            # Initialize harmonic parameters as PyTorch tensors with gradients enabled\n",
    "            amplitudes = torch.randn(harmonic_cache.num_harmonics, requires_grad=True)\n",
    "            frequencies = torch.randn(harmonic_cache.num_harmonics, requires_grad=True)\n",
    "            phases = torch.randn(harmonic_cache.num_harmonics, requires_grad=True)\n",
    "\n",
    "            # Define the optimizer for these parameters\n",
    "            optimizer = optim.Adam([amplitudes, frequencies, phases], lr=learning_rate)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate the predicted embedding from the harmonic parameters\n",
    "            # Using a similar approach as decode_embedding but with PyTorch tensors\n",
    "            reconstructed = torch.zeros(harmonic_cache.original_dim, requires_grad=False)\n",
    "\n",
    "            window_size = harmonic_cache.original_dim // harmonic_cache.num_harmonics\n",
    "            for h in range(harmonic_cache.num_harmonics):\n",
    "                start, end = h * window_size, min((h + 1) * window_size, harmonic_cache.original_dim)\n",
    "                wave = amplitudes[h] * torch.sin(2 * torch.pi * frequencies[h] * 1.0 + phases[h])\n",
    "                reconstructed[start:end] = wave\n",
    "\n",
    "            # Compute the loss between the target embedding and the reconstructed embedding\n",
    "            loss = mse_loss(reconstructed, embedding_tensor)\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(tokens)}\")\n",
    "\n",
    "train_loop(tokens, epochs=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
