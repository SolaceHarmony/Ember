{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3114983937.py, line 97)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 97\u001b[0;36m\u001b[0m\n\u001b[0;31m    return self.state    def __init__(self,\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class AttentionState:\n",
    "    temporal_weight: float = 0.0    # Recent history importance\n",
    "    causal_weight: float = 0.0      # Prediction accuracy impact\n",
    "    novelty_weight: float = 0.0     # Curiosity factor\n",
    "    \n",
    "    def compute_total(self) -> float:\n",
    "        return (\n",
    "            self.temporal_weight +\n",
    "            self.causal_weight +\n",
    "            self.novelty_weight\n",
    "        ) / 3.0\n",
    "\n",
    "class CausalAttention:\n",
    "    def __init__(self,\n",
    "                 decay_rate: float = 0.1,\n",
    "                 novelty_threshold: float = 0.3,\n",
    "                 memory_length: int = 100):\n",
    "        self.states: Dict[int, AttentionState] = {}\n",
    "        self.history: List[Tuple[int, float]] = []\n",
    "        self.decay_rate = decay_rate\n",
    "        self.novelty_threshold = novelty_threshold\n",
    "        self.memory_length = memory_length\n",
    "    \n",
    "    def update(self, neuron_id: int, prediction_error: float, current_state: float, target_state: float) -> float:\n",
    "        state = self.states.get(neuron_id, AttentionState())\n",
    "        \n",
    "        # Update temporal weight\n",
    "        temporal_decay = np.exp(-self.decay_rate * len(self.history))\n",
    "        state.temporal_weight = current_state * temporal_decay\n",
    "        \n",
    "        # Update causal weight\n",
    "        prediction_accuracy = 1.0 - min(abs(prediction_error), 1.0)\n",
    "        state.causal_weight = prediction_accuracy\n",
    "        \n",
    "        # Update novelty weight\n",
    "        novelty = abs(target_state - current_state)\n",
    "        if novelty > self.novelty_threshold:\n",
    "            state.novelty_weight = novelty\n",
    "        else:\n",
    "            state.novelty_weight *= (1 - self.decay_rate)\n",
    "        \n",
    "        # Store updated state\n",
    "        self.states[neuron_id] = state\n",
    "        \n",
    "        # Update history\n",
    "        total_attention = state.compute_total()\n",
    "        self.history.append((neuron_id, total_attention))\n",
    "        if len(self.history) > self.memory_length:\n",
    "            self.history.pop(0)\n",
    "        \n",
    "        return total_attention\n",
    "\n",
    "class LTCNeuronWithAttention:\n",
    "    def __init__(self, \n",
    "                 neuron_id: int,\n",
    "                 tau: float = 1.0):\n",
    "        self.id = neuron_id\n",
    "        self.tau = tau\n",
    "        self.state = 0.0\n",
    "        self.attention = CausalAttention()\n",
    "        self.last_prediction = 0.0\n",
    "        \n",
    "    def update(self, input_signal: float, dt: float) -> float:\n",
    "        # Calculate prediction error\n",
    "        prediction_error = input_signal - self.last_prediction\n",
    "        \n",
    "        # Update attention \n",
    "        attention_value = self.attention.update(\n",
    "            neuron_id=self.id,\n",
    "            prediction_error=prediction_error,\n",
    "            current_state=self.state,\n",
    "            target_state=input_signal\n",
    "        )\n",
    "        \n",
    "        # Modulate time constant\n",
    "        effective_tau = self.tau * (1.0 - 0.3 * attention_value)\n",
    "        \n",
    "        # Update LTC dynamics\n",
    "        d_state = (1.0/effective_tau) * (\n",
    "            input_signal * (1.0 + attention_value) - self.state\n",
    "        ) * dt\n",
    "        \n",
    "        self.state += d_state\n",
    "        self.last_prediction = self.state\n",
    "        \n",
    "        return self.state\n",
    "class AttentiveNetwork:\n",
    "    def __init__(self, num_neurons: int = 3, base_tau: float = 1.0):\n",
    "        # Create main neurons\n",
    "        self.neurons = [ LTCNeuronWithAttention(i, base_tau * (1 + 0.2 * i)) for i in range(num_neurons)]\n",
    "        self.num_neurons = num_neurons\n",
    "        # Initialize histories for visualization\n",
    "        self.state_history = []\n",
    "        self.attention_history = []\n",
    "        self.input_history = []\n",
    "        self.time_steps = []\n",
    "        self.current_step = 0\n",
    "\n",
    "    def update_step(self, input_signals: List[float], dt: float = 0.1) -> List[float]:\n",
    "        states = []\n",
    "        attention_values = []\n",
    "        \n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            # Get input for this neuron\n",
    "            input_signal = input_signals[i] if i < len(input_signals) else 0.0\n",
    "            \n",
    "            # Update neuron state\n",
    "            state = neuron.update(input_signal, dt)\n",
    "            states.append(state)\n",
    "            \n",
    "            # Get attention value\n",
    "            attention = neuron.attention.states.get(\n",
    "                neuron.id, AttentionState()).compute_total()\n",
    "            attention_values.append(attention)\n",
    "\n",
    "        # Record histories\n",
    "        self.state_history.append(states)\n",
    "        self.attention_history.append(attention_values)\n",
    "        self.input_history.append(input_signals)\n",
    "        self.time_steps.append(self.current_step)\n",
    "        self.current_step += 1\n",
    "            \n",
    "        return states\n",
    "\n",
    "    def plot_results(self):\n",
    "        \"\"\"Visualize network behavior\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10))\n",
    "        \n",
    "        # Convert histories to numpy arrays for easier plotting\n",
    "        states = np.array(self.state_history)\n",
    "        attention = np.array(self.attention_history)\n",
    "        inputs = np.array(self.input_history)\n",
    "        \n",
    "        # Plot states\n",
    "        for i in range(self.num_neurons):\n",
    "            ax1.plot(self.time_steps, states[:, i], label=f'Neuron {i}')\n",
    "        ax1.set_title('Neuron States')\n",
    "        ax1.set_xlabel('Time Step')\n",
    "        ax1.set_ylabel('State')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Plot attention\n",
    "        for i in range(self.num_neurons):\n",
    "            ax2.plot(self.time_steps, attention[:, i], label=f'Attention {i}')\n",
    "        ax2.set_title('Attention Values')\n",
    "        ax2.set_xlabel('Time Step')\n",
    "        ax2.set_ylabel('Attention')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        # Plot inputs\n",
    "        for i in range(self.num_neurons):\n",
    "            ax3.plot(self.time_steps, inputs[:, i], label=f'Input {i}')\n",
    "        ax3.set_title('Input Signals')\n",
    "        ax3.set_xlabel('Time Step')\n",
    "        ax3.set_ylabel('Input Value')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def generate_input_patterns(num_steps: int, num_neurons: int, \n",
    "                        freq: float = 0.1, dt: float = 0.1) -> List[List[float]]:\n",
    "    \"\"\"Generate structured input patterns for testing\"\"\"\n",
    "    input_patterns = []\n",
    "    for step in range(num_steps):\n",
    "        t = step * dt\n",
    "        inputs = [\n",
    "            0.5 + 0.5 * np.sin(2 * np.pi * freq * t),  # Sine wave\n",
    "            0.5 + 0.5 * np.cos(2 * np.pi * freq * t),  # Cosine wave\n",
    "            0.5 + 0.25 * (np.sin(2 * np.pi * freq * 2 * t) +  # Complex pattern\n",
    "                        np.cos(2 * np.pi * freq * t))\n",
    "        ][:num_neurons]  # Truncate to number of neurons\n",
    "        input_patterns.append(inputs)\n",
    "    return input_patterns\n",
    "\n",
    "def run_simulation(network: AttentiveNetwork, \n",
    "                  steps: int = 200, \n",
    "                  dt: float = 0.1,\n",
    "                  input_freq: float = 0.05) -> None:\n",
    "    \"\"\"Run simulation with generated input patterns\"\"\"\n",
    "    input_patterns = generate_input_patterns(steps, network.num_neurons, \n",
    "                                          freq=input_freq, dt=dt)\n",
    "    \n",
    "    print(\"Running simulation...\")\n",
    "    for inputs in input_patterns:\n",
    "        network.update_step(inputs, dt)\n",
    "    \n",
    "    print(\"Plotting results...\")\n",
    "    network.plot_results()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create and run network\n",
    "    network = AttentiveNetwork(num_neurons=3)\n",
    "    run_simulation(network)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
